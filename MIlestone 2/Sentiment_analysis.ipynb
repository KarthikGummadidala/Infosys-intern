{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Define the API Key and endpoint\n",
        "API_KEY = \"6d1fda2210ea41c69edf6a9ce33f4b13\"\n",
        "NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "# Define the search parameters\n",
        "query = \"supply chain disruption\"\n",
        "params = {\n",
        "    \"q\": \"supply chain disruption\",\n",
        "    \"apiKey\": API_KEY,\n",
        "    \"pageSize\": 100,\n",
        "    \"sortBy\": \"relevance\",\n",
        "}\n",
        "\n",
        "# Fetch news articles\n",
        "response = requests.get(NEWS_API_URL, params=params)\n",
        "articles = response.json().get(\"articles\", [])\n",
        "\n",
        "# Define the risk factors\n",
        "risk_factors = [\n",
        "    \"Logistical\",\n",
        "    \"Natural Disasters\",\n",
        "    \"Market\",\n",
        "    \"Regulatory\",\n",
        "    \"Economic\",\n",
        "    \"Geopolitical\",\n",
        "    \"Customs Delays\",\n",
        "    \"Supply chain disruptions\",\n",
        "    \"Global competition\",\n",
        "    \"Rising labor costs\",\n",
        "]\n",
        "\n",
        "# Load a pre-trained model from Hugging Face\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the risk factors\n",
        "risk_factor_embeddings = model.encode(risk_factors, convert_to_tensor=True)\n",
        "\n",
        "# Process articles and calculate risk factor scores\n",
        "examples = []\n",
        "for article in articles[:200]:  # Limit to 200 articles\n",
        "    title = article.get(\"title\", \"No Title\")\n",
        "    description = article.get(\"description\", \"No Description\")\n",
        "    example_text = f\"{title}: {description}\"\n",
        "\n",
        "    # Encode the example text\n",
        "    example_embedding = model.encode(example_text, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate similarity scores with risk factors\n",
        "    similarities = util.cos_sim(example_embedding, risk_factor_embeddings)[0]\n",
        "\n",
        "    # Convert similarity scores to a list of decimal values\n",
        "    risk_scores = similarities.tolist()\n",
        "\n",
        "    # Determine the final risk factor and its value (highest similarity score)\n",
        "    max_score_index = risk_scores.index(max(risk_scores))\n",
        "    final_risk_factor = risk_factors[max_score_index]\n",
        "    final_risk_value = max(risk_scores)\n",
        "\n",
        "    # Append the example with risk scores, final risk factor, and its value\n",
        "    examples.append((example_text, risk_scores, final_risk_factor, final_risk_value))\n",
        "\n",
        "# Write to CSV file\n",
        "csv_file_path = \"news_import_export_risks_file.csv\"\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow([\"Example\"] + risk_factors + [\"Final Risk Factor\", \"Final Risk Value\"])\n",
        "    # Write rows with risk scores, final risk factor, and its value\n",
        "    for example, scores, final_risk_factor, final_risk_value in examples:\n",
        "        writer.writerow([example] + scores + [final_risk_factor, final_risk_value])\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully with {len(examples)} rows.\")\n",
        "\n",
        "\n",
        "\n",
        "# Load a pre-trained model from Hugging Face\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the risk factors\n",
        "risk_factor_embeddings = model.encode(risk_factors, convert_to_tensor=True)\n",
        "\n",
        "# Process articles and calculate risk factor scores\n",
        "examples = []\n",
        "for article in articles[:200]:  # Limit to 200 articles\n",
        "    title = article.get(\"title\", \"No Title\")\n",
        "    description = article.get(\"description\", \"No Description\")\n",
        "    example_text = f\"{title}: {description}\"\n",
        "\n",
        "    # Encode the example text\n",
        "    example_embedding = model.encode(example_text, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate similarity scores with risk factors\n",
        "    similarities = util.cos_sim(example_embedding, risk_factor_embeddings)[0]\n",
        "\n",
        "    # Convert similarity scores to a list of decimal values\n",
        "    risk_scores = similarities.tolist()\n",
        "\n",
        "    # Determine the final risk value (highest similarity score)\n",
        "    final_risk_value = max(risk_scores)\n",
        "\n",
        "    # Append the example with risk scores and the final risk value\n",
        "    examples.append((example_text, risk_scores, final_risk_value))\n",
        "\n",
        "# Write to CSV file\n",
        "csv_file_path = \"news_import_export_risks_file.csv\"\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow([\"Example\"] + risk_factors + [\"Final Risk Value\"])\n",
        "    # Write rows with risk scores and the final risk value\n",
        "    for example, scores, final_risk_value in examples:\n",
        "        writer.writerow([example] + scores + [final_risk_value])\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully with {len(examples)} rows.\")\n"
      ],
      "metadata": {
        "id": "xQFyWLJofxfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308789a5-ce2b-4a2c-e5cb-79a74a58a15d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'news_import_export_risks_file.csv' created successfully with 100 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file_path = \"/content/news_import_export_risks_file.csv\"\n",
        "\n",
        "# Load and display the CSV file\n",
        "df = pd.read_csv(csv_file_path)\n",
        "print(df.head())  # Display the first 5 rows\n"
      ],
      "metadata": {
        "id": "c9xiuMFufz_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40b6426-5c4e-479c-8b6b-1da65250a8c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Example  Logistical  \\\n",
            "0  The Most Dangerous People on the Internet in 2...    0.136250   \n",
            "1  Geopolitics and AI will affect the chip indust...    0.173755   \n",
            "2  Moselle Lock Closure: How To Mitigate Supply C...    0.199159   \n",
            "3  NATO is working to reroute data through space,...    0.185938   \n",
            "4  The FDA says the shortage of popular weight lo...    0.123055   \n",
            "\n",
            "   Natural Disasters    Market  Regulatory  Economic  Geopolitical  \\\n",
            "0           0.312669  0.104376    0.087930  0.113044      0.212882   \n",
            "1           0.135852  0.234857    0.213022  0.325603      0.426412   \n",
            "2           0.230963  0.149327    0.195710  0.199763      0.077721   \n",
            "3           0.146406  0.072456    0.053196  0.070176      0.319405   \n",
            "4          -0.046860  0.149906    0.127936  0.119104      0.047725   \n",
            "\n",
            "   Customs Delays  Supply chain disruptions  Global competition  \\\n",
            "0        0.054294                  0.208997            0.218297   \n",
            "1        0.085301                  0.291086            0.396363   \n",
            "2        0.242192                  0.530209            0.191239   \n",
            "3        0.157852                  0.157077            0.231974   \n",
            "4        0.075341                  0.192803            0.140797   \n",
            "\n",
            "   Rising labor costs         Final Risk Factor  Final Risk Value  \n",
            "0            0.025542         Natural Disasters          0.312669  \n",
            "1            0.167828              Geopolitical          0.426412  \n",
            "2            0.240459  Supply chain disruptions          0.530209  \n",
            "3            0.006413              Geopolitical          0.319405  \n",
            "4            0.090782  Supply chain disruptions          0.192803  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download the VADER lexicon if not already done\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment_data = []\n",
        "for article in articles[:200]:  # Limit to 200 articles\n",
        "    title = article.get(\"title\", \"No Title\")\n",
        "    description = article.get(\"description\", \"No Description\")\n",
        "    text = f\"{title}: {description}\"\n",
        "\n",
        "    # Analyze sentiment\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    sentiment_category = (\n",
        "        \"Positive\"\n",
        "        if sentiment_scores[\"compound\"] > 0.05\n",
        "        else \"Negative\"\n",
        "        if sentiment_scores[\"compound\"] < -0.05\n",
        "        else \"Neutral\"\n",
        "    )\n",
        "\n",
        "    # Append sentiment data\n",
        "    sentiment_data.append((text, sentiment_scores, sentiment_category))\n",
        "\n",
        "# Write sentiment data to a CSV file\n",
        "csv_file_path = \"news_sentiment_analysis.csv\"\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow([\"Text\", \"Positive\", \"Neutral\", \"Negative\", \"Compound\", \"Category\"])\n",
        "    # Write rows with sentiment scores and categories\n",
        "    for text, scores, category in sentiment_data:\n",
        "        writer.writerow([text, scores[\"pos\"], scores[\"neu\"], scores[\"neg\"], scores[\"compound\"], category])\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully with {len(sentiment_data)} rows.\")\n"
      ],
      "metadata": {
        "id": "0qQ3jDqXmSwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}